# Step 1: RAG Dataset Setup and Exploration

The RAG dataset we use for this project can be accessed via https://huggingface.co/datasets/rag-datasets/rag-mini-wikipedia. It is a text dataset splitted into two subsets: a) text-corpus, which contains 3,200 sentences or short terms excerpted from Wikipedia that describes facts or subjects; and b) question-answer, which contains 918 pairs of questions and ground-truth answers that are derived from the contents of text-corpus. Each subset has its own set of identifiers. Sample inspection confirmed consistent formatting across entries.

The passages in the text-corpus subset show no null or duplicate values. The lengths of text passages range from 1 to 2515, with an average length of 389.85 and a median of 299. Most entries fall into a moderate range of characters. However, the relatively large variation in passage length and the small amount of overly long passages will influence our chunking strategy in later phases. Without segmentation, overly long passages would exceed model input limits, while extremely short passages might reduce retrieval precision. Planning for fixed-size token windows with overlap is therefore essential.

From an infrastructure perspective, the dataset’s scale suggests that both FAISS and Milvus Lite are viable as vector stores, with little risk of exceeding memory on common configurations. We will continue to use Milvus Lite to store word vectors, which is in accordance with the example starter code. Exporting the dataset to local Parquet format will further support modular workflows and enable rapid reloading and integration with preprocessing pipelines. Furthermore, a quick connection test proves that ChatGPT 4o-mini can be connected to the codespace without errors using API. Together, these observations provide a clear understanding of the dataset’s composition and inform the technical planning needed for building an effective baseline RAG system in the next step.
