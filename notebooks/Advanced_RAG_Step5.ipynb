{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e3ee951c6ceb46ba9a52f04e14548d59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61b80d43e40b44cdb93e0a3430866198",
              "IPY_MODEL_40667edb78fe4878801a36a2ff1e1314",
              "IPY_MODEL_682b0c8167d14350a8accf853b88b4c4"
            ],
            "layout": "IPY_MODEL_05da87d6227a495da7523da6b2291477"
          }
        },
        "61b80d43e40b44cdb93e0a3430866198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7882635efced4fe3b3d87527e6a1f436",
            "placeholder": "​",
            "style": "IPY_MODEL_84351f9e12214ee598b5e41bd58ed1bd",
            "value": "Batches: 100%"
          }
        },
        "40667edb78fe4878801a36a2ff1e1314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3348f170824a0fb750da383f82f405",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5f67fe87f3f490a86041851b913c16c",
            "value": 64
          }
        },
        "682b0c8167d14350a8accf853b88b4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2744e0c2dbf4b65a15e04de9fdbb1da",
            "placeholder": "​",
            "style": "IPY_MODEL_2faff7ed297140e9941140a86662f870",
            "value": " 64/64 [00:04&lt;00:00, 42.13it/s]"
          }
        },
        "05da87d6227a495da7523da6b2291477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7882635efced4fe3b3d87527e6a1f436": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84351f9e12214ee598b5e41bd58ed1bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b3348f170824a0fb750da383f82f405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5f67fe87f3f490a86041851b913c16c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2744e0c2dbf4b65a15e04de9fdbb1da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2faff7ed297140e9941140a86662f870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Advanced RAG Pipeline Implementation\n",
        "\n",
        "Features:\n",
        " 1) Multi-Query Expansion (MQE) with Flan-T5 → broaden recall\n",
        " 2) Cross-Encoder reranking (ms-marco MiniLM-L-6-v2) → improve precision\n",
        "\n",
        " Pipeline:\n",
        "  - Build FAISS index using all-MiniLM-L6-v2 embeddings\n",
        "  - For each question: generate rewrites → retrieve candidates\n",
        "  - Rerank with CrossEncoder → compose grounded context (citations)\n",
        "  - Generate final answer (Flan-T5 by default; OpenAI optional)\n",
        "  - (Optional) small SQuAD evaluation for sanity check"
      ],
      "metadata": {
        "id": "N2NZdVd0ef2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0) Configs"
      ],
      "metadata": {
        "id": "xmDCX8PiesWw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBba9kikeE1W",
        "outputId": "79ec9fdd-d9e8-4182-820b-51d80b82bc58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config: {'embed_model': 'sentence-transformers/all-MiniLM-L6-v2', 'chunk_max_chars': 600, 'batch_size': 64, 'retrieve_k_per_query': 20, 'n_rewrites': 3, 'final_top_k': 5, 'max_ctx_chars': 2000, 'use_openai': False}\n"
          ]
        }
      ],
      "source": [
        "CFG = {\n",
        "    \"embed_model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # 384-dim embeddings\n",
        "    \"chunk_max_chars\": 600,        # simple character-based chunking\n",
        "    \"batch_size\": 64,\n",
        "    \"retrieve_k_per_query\": 20,    # candidates per (query/rewrite)\n",
        "    \"n_rewrites\": 3,               # number of MQE rewrites\n",
        "    \"final_top_k\": 5,              # passages kept after CE reranking\n",
        "    \"max_ctx_chars\": 2000,         # context budget for the prompt\n",
        "    \"use_openai\": False,           # switch to True to use OpenAI (needs OPENAI_API_KEY)\n",
        "}\n",
        "print(\"Config:\", CFG)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Install and import packages"
      ],
      "metadata": {
        "id": "JfuB82U0e4ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, subprocess, pkgutil, os, time\n",
        "def ensure(pkg, pip_name=None):\n",
        "    pip_name = pip_name or pkg\n",
        "    if pkgutil.find_loader(pkg) is None:\n",
        "        print(\"Installing:\", pip_name)\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name])\n",
        "    else:\n",
        "        print(\"OK:\", pkg)\n",
        "\n",
        "ensure(\"datasets\", \"datasets\")\n",
        "ensure(\"sentence_transformers\", \"sentence-transformers\")\n",
        "ensure(\"transformers\", \"transformers\")\n",
        "ensure(\"faiss\", \"faiss-cpu\")\n",
        "ensure(\"numpy\", \"numpy\")\n",
        "ensure(\"pandas\", \"pandas\")\n",
        "ensure(\"evaluate\", \"evaluate\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import faiss, numpy as np, pandas as pd\n",
        "import evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msNsMZXKfNpv",
        "outputId": "32484ada-b417-4785-ebc6-de964a12f0a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1376048660.py:4: DeprecationWarning: 'pkgutil.find_loader' is deprecated and slated for removal in Python 3.14; use importlib.util.find_spec() instead\n",
            "  if pkgutil.find_loader(pkg) is None:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: datasets\n",
            "OK: sentence_transformers\n",
            "OK: transformers\n",
            "Installing: faiss-cpu\n",
            "OK: numpy\n",
            "OK: pandas\n",
            "Installing: evaluate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Dataset loading and chunking"
      ],
      "metadata": {
        "id": "QljMtwi4fvZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_corpus = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\")\n",
        "print(\"Corpus Dataset:\", ds_corpus)\n",
        "corpus = ds_corpus[\"passages\"]\n",
        "N_DOCS = 1000\n",
        "corpus = corpus.select(range(min(N_DOCS, len(corpus))))\n",
        "\n",
        "ds_qa = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\")\n",
        "print(\"QA Dataset:\", ds_qa)\n",
        "qa = ds_qa[\"test\"]\n",
        "\n",
        "\n",
        "def simple_chunks(text, max_chars=CFG[\"chunk_max_chars\"]):\n",
        "    if not text:\n",
        "        return []\n",
        "    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
        "\n",
        "docs = []\n",
        "for i, row in enumerate(corpus):\n",
        "    t = row.get(\"text\") or row.get(\"passage\") or \"\"\n",
        "    for j, ch in enumerate(simple_chunks(t, CFG[\"chunk_max_chars\"])):\n",
        "        docs.append({\"id\": f\"{i}-{j}\", \"text\": ch})\n",
        "\n",
        "print(\"Total chunks:\", len(docs))\n",
        "print(\"Example chunk:\", docs[0][\"id\"], docs[0][\"text\"][:120], \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgVb7hPufzOb",
        "outputId": "195b6f25-5293-4e58-b023-428d40682a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Dataset: DatasetDict({\n",
            "    passages: Dataset({\n",
            "        features: ['passage', 'id'],\n",
            "        num_rows: 3200\n",
            "    })\n",
            "})\n",
            "QA Dataset: DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['question', 'answer', 'id'],\n",
            "        num_rows: 918\n",
            "    })\n",
            "})\n",
            "Total chunks: 4046\n",
            "Example chunk: 0-0 Uruguay (official full name in  ; pron.  , Eastern Republic of  Uruguay) is a country located in the southeastern part o ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Embedding Index (FAISS, cosine via IP on normalized vecs)"
      ],
      "metadata": {
        "id": "AnCKJ_SkgwMQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e3ee951c6ceb46ba9a52f04e14548d59",
            "61b80d43e40b44cdb93e0a3430866198",
            "40667edb78fe4878801a36a2ff1e1314",
            "682b0c8167d14350a8accf853b88b4c4",
            "05da87d6227a495da7523da6b2291477",
            "7882635efced4fe3b3d87527e6a1f436",
            "84351f9e12214ee598b5e41bd58ed1bd",
            "2b3348f170824a0fb750da383f82f405",
            "e5f67fe87f3f490a86041851b913c16c",
            "b2744e0c2dbf4b65a15e04de9fdbb1da",
            "2faff7ed297140e9941140a86662f870"
          ]
        },
        "id": "95b97f7c",
        "outputId": "de354bd8-c507-4d4e-8bf9-15fa599a0024"
      },
      "source": [
        "embed_model = SentenceTransformer(CFG[\"embed_model\"])\n",
        "texts = [d[\"text\"] for d in docs]\n",
        "Emb = embed_model.encode(\n",
        "    texts, batch_size=CFG[\"batch_size\"], show_progress_bar=True,\n",
        "    normalize_embeddings=True, truncate=True\n",
        ").astype(\"float32\")\n",
        "\n",
        "index = faiss.IndexFlatIP(Emb.shape[1])\n",
        "index.add(Emb)\n",
        "id_map = np.array([d[\"id\"] for d in docs])\n",
        "print(\"Index built | dim:\", Emb.shape[1], \"| num_vecs:\", index.ntotal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/64 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3ee951c6ceb46ba9a52f04e14548d59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index built | dim: 384 | num_vecs: 4046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Advanced feature 1: Query Rewriting and Multi-Query Expansion (MQE)\n",
        "- Use Flan-T5 as lightweight transformer\n",
        "- Featuring diversified rewrite and deduplication"
      ],
      "metadata": {
        "id": "bYU2_8E8hTht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "gen_tok = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "gen_mdl = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "generator = pipeline(\"text2text-generation\", model=gen_mdl, tokenizer=gen_tok)\n",
        "\n",
        "def _clean(q: str) -> str:\n",
        "    q = q.strip()\n",
        "    q = re.sub(r\"\\s+\", \" \", q)\n",
        "    q = q.replace(\"’\",\"'\").replace(\"“\",\"\\\"\").replace(\"”\",\"\\\"\").replace(\"–\",\"-\").replace(\"—\",\"-\")\n",
        "    return q\n",
        "\n",
        "def _semantic_dedup(cands, embed_model, sim_thresh=0.95, keep_n=3):\n",
        "    \"\"\"Deduplication using sentence-transformer\"\"\"\n",
        "    uniq = []\n",
        "    if not cands:\n",
        "        return uniq\n",
        "    vecs = embed_model.encode(cands, normalize_embeddings=True).astype(\"float32\")\n",
        "    for i, c in enumerate(cands):\n",
        "        if not uniq:\n",
        "            uniq.append((c, vecs[i]));\n",
        "            continue\n",
        "        sims = [float(vecs[i] @ v) for _, v in uniq]\n",
        "        if max(sims) < sim_thresh:\n",
        "            uniq.append((c, vecs[i]))\n",
        "        if len(uniq) >= keep_n:\n",
        "            break\n",
        "    return [t[0] for t in uniq]\n",
        "\n",
        "def make_rewrites(question, n=CFG[\"n_rewrites\"]):\n",
        "    \"\"\"Generate n diverse rewrites of the question\"\"\"\n",
        "    q0 = _clean(question)\n",
        "    prompt = (\n",
        "        \"Rewrite the question into diverse, retrieval-friendly queries. \"\n",
        "        \"Return short alternatives, one per line, no numbering.\\n\\n\"\n",
        "        f\"Question: {q0}\"\n",
        "    )\n",
        "\n",
        "    # beams >= n to ensure num_return_sequences ≤ num_beams\n",
        "    N = max(n, 6)\n",
        "    try:\n",
        "        outs = generator(\n",
        "            prompt,\n",
        "            max_new_tokens=96,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            num_beams=N,                # beams >= num_return_sequences\n",
        "            num_return_sequences=N,     # Return N candidates at a time\n",
        "            no_repeat_ngram_size=3,     # Limit repetition\n",
        "        )\n",
        "    except ValueError as e:\n",
        "\n",
        "        N = max(n, 4)\n",
        "        outs = generator(\n",
        "            prompt,\n",
        "            max_new_tokens=96,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            num_beams=N,\n",
        "            num_return_sequences=N,\n",
        "            no_repeat_ngram_size=3,\n",
        "        )\n",
        "\n",
        "    # Summarization\n",
        "    raw_lines = []\n",
        "    for o in outs:\n",
        "        for ln in o[\"generated_text\"].splitlines():\n",
        "            ln = _clean(ln)\n",
        "            if ln:\n",
        "                raw_lines.append(ln)\n",
        "\n",
        "    # Dedupe candidates identical to original questions\n",
        "    uniq_lines, seen = [], set()\n",
        "    for s in raw_lines:\n",
        "        if s.lower() == q0.lower():\n",
        "            continue\n",
        "        if s.lower() in seen:\n",
        "            continue\n",
        "        seen.add(s.lower())\n",
        "        uniq_lines.append(s)\n",
        "\n",
        "    # Semantic dedupe\n",
        "    diverse = _semantic_dedup(uniq_lines, embed_model, sim_thresh=0.95, keep_n=max(n,1))\n",
        "\n",
        "    while len(diverse) < n:\n",
        "        diverse.append(q0)\n",
        "\n",
        "    return diverse[:n]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOwyPMXLha4S",
        "outputId": "ec96ec02-026a-4a29-f2eb-63bb53c25f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Candidate retrieval across rewrites"
      ],
      "metadata": {
        "id": "yHEgzIdhhitW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def faiss_search(query, top_k):\n",
        "    qv = embed_model.encode([query], normalize_embeddings=True).astype(\"float32\")\n",
        "    D, I = index.search(qv, top_k)\n",
        "    return list(zip(I[0].tolist(), D[0].tolist()))  # (idx, score)\n",
        "\n",
        "def gather_candidates(question):\n",
        "    \"\"\"\n",
        "    For original question + rewrites, retrieve candidates and deduplicate by passage index.\n",
        "    Keep max similarity for each idx; return list sorted by vector sim desc.\n",
        "    \"\"\"\n",
        "    rewrites = make_rewrites(question, CFG[\"n_rewrites\"])\n",
        "    cand = []\n",
        "    for q in [question] + rewrites:\n",
        "        hits = faiss_search(q, CFG[\"retrieve_k_per_query\"])\n",
        "        for idx, s in hits:\n",
        "            cand.append((idx, s, q))\n",
        "    best = {}\n",
        "    for idx, s, q in cand:\n",
        "        if idx not in best or s > best[idx][0]:\n",
        "            best[idx] = (s, q)\n",
        "    out = [(k, v[0], v[1]) for k, v in best.items()]\n",
        "    out.sort(key=lambda x: x[1], reverse=True)\n",
        "    return out, rewrites\n"
      ],
      "metadata": {
        "id": "BVc2AVxYhmq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Advanced feature 2: Cross-Encoder Reranking\n",
        "- A strong MS MARCO cross-encoder for (query, passage) scoring\n",
        "- Explicit truncation to 768 tokens"
      ],
      "metadata": {
        "id": "Y-aSTrRXhouu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import CrossEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Cross encoder and the corresponding auto tokenizer\n",
        "ce_model_id = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        "ce_tok = AutoTokenizer.from_pretrained(ce_model_id)\n",
        "\n",
        "# Manually defined max length\n",
        "reranker = CrossEncoder(ce_model_id, max_length=768)\n",
        "\n",
        "def truncate_by_tokens(text: str, tokenizer, max_len: int) -> str:\n",
        "    \"\"\"Truncate text to max_len\"\"\"\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    if len(ids) > max_len:\n",
        "        ids = ids[:max_len]\n",
        "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
        "\n",
        "def rerank(\n",
        "    question: str,\n",
        "    candidates,                   # list of (idx, vec_sim, rewrite_used)\n",
        "    top_k: int = CFG[\"final_top_k\"],\n",
        "    q_budget: int = 64,           # Token budget for questions\n",
        "    p_budget: int = 704            # Token budget for text passage\n",
        "):\n",
        "    \"\"\"\n",
        "    Token-based trunking for questions and text\n",
        "    return：list[(idx, rerank_score)]\n",
        "    \"\"\"\n",
        "    # 1) Truncate questions\n",
        "    q_trunc = truncate_by_tokens(question, ce_tok, max_len=q_budget)\n",
        "\n",
        "    # 2) Perform pairing and truncate passage to p-budget\n",
        "    pairs, idxs = [], []\n",
        "    for idx, _, _ in candidates:\n",
        "        p_text = docs[idx][\"text\"]\n",
        "        p_trunc = truncate_by_tokens(p_text, ce_tok, max_len=p_budget)\n",
        "        pairs.append((q_trunc, p_trunc))\n",
        "        idxs.append(idx)\n",
        "\n",
        "    if not pairs:\n",
        "        return []\n",
        "\n",
        "    # 3) Cross-encoder ranking\n",
        "    scores = reranker.predict(\n",
        "        pairs,\n",
        "        convert_to_numpy=True,\n",
        "        show_progress_bar=False,\n",
        "        batch_size=64,\n",
        "    )\n",
        "\n",
        "    # 4) Fetch top-k\n",
        "    order = np.argsort(-scores)[:top_k]\n",
        "    chosen = [(int(idxs[i]), float(scores[i])) for i in order]\n",
        "    return chosen"
      ],
      "metadata": {
        "id": "U60aoXh1hyXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Build context with citations & generate"
      ],
      "metadata": {
        "id": "RI3L9UNdh6cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_context_with_citations(chosen, budget=CFG[\"max_ctx_chars\"]):\n",
        "    parts, cites, used = [], [], 0\n",
        "    for idx, score in chosen:\n",
        "        text = docs[idx][\"text\"]\n",
        "        pid = id_map[idx]\n",
        "        tag = f\"[{pid} | {score:.3f}]\"\n",
        "        snippet = f\"{tag}\\n{text}\"\n",
        "        if used + len(snippet) > budget:\n",
        "            parts.append(snippet[: max(0, budget - used)])\n",
        "            cites.append({\"id\": pid, \"score\": float(score)})\n",
        "            break\n",
        "        parts.append(snippet)\n",
        "        cites.append({\"id\": pid, \"score\": float(score)})\n",
        "        used += len(snippet)\n",
        "    return \"\\n\\n\".join(parts), cites\n",
        "\n",
        "def persona_prompt(context, question):\n",
        "    return (\n",
        "        \"You are a concise encyclopedia editor. Answer USING ONLY the context; \"\n",
        "        \"if insufficient, reply 'I don't know.' Include short inline source tags if helpful.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "def answer_advanced(question):\n",
        "    # 1) MQE + vector recall\n",
        "    candidates, rewrites = gather_candidates(question)\n",
        "    # 2) Cross-encoder rerank\n",
        "    chosen = rerank(question, candidates, top_k=CFG[\"final_top_k\"])\n",
        "    # 3) Grounded context with citations\n",
        "    ctx, citations = build_context_with_citations(chosen, CFG[\"max_ctx_chars\"])\n",
        "\n",
        "    if CFG[\"use_openai\"] and os.getenv(\"OPENAI_API_KEY\"): #Optional: use ChatGPT 4o if valid API key available\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI()\n",
        "        prompt = persona_prompt(ctx, question)\n",
        "        resp = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.2, max_tokens=256\n",
        "        )\n",
        "        ans = resp.choices[0].message.content.strip()\n",
        "    else:\n",
        "        prompt = persona_prompt(ctx, question)\n",
        "        ans = generator(prompt, max_new_tokens=256)[0][\"generated_text\"].strip()\n",
        "\n",
        "    return ans, citations, rewrites"
      ],
      "metadata": {
        "id": "mjb0M4Ath_GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Demonstration of advanced features"
      ],
      "metadata": {
        "id": "NleHSJMziB4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_q = qa[0][\"question\"]\n",
        "print(\"Question:\", sample_q)\n",
        "ans, cites, rewrites = answer_advanced(sample_q)\n",
        "print(\"\\nRewrites:\\n\", \"\\n\".join(rewrites))\n",
        "print(\"\\nAnswer:\\n\", ans)\n",
        "print(\"\\nCitations:\", cites[:CFG[\"final_top_k\"]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VSvEnIpiE8C",
        "outputId": "90ce023c-3f5e-45e8-e747-18e06eccb4e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
            "\n",
            "Rewrites:\n",
            " Is Abraham Lincoln the sixteenth President of the United States?\n",
            "Abraham Lincoln was the sixteenth President of the United States.\n",
            "Was Abraham Lincoln the sixteenth President of the United States?\n",
            "\n",
            "Answer:\n",
            " Abraham Lincoln (February 12, 1809 â April 15, 1865) was the sixteenth President of the United States, serving from March 4, 1861 until his assassination.\n",
            "\n",
            "Citations: [{'id': np.str_('278-0'), 'score': 10.30893611907959}, {'id': np.str_('319-0'), 'score': 8.121994972229004}, {'id': np.str_('198-0'), 'score': -0.44946980476379395}, {'id': np.str_('383-0'), 'score': -0.7347850203514099}, {'id': np.str_('281-0'), 'score': -0.9723397493362427}]\n"
          ]
        }
      ]
    }
  ]
}