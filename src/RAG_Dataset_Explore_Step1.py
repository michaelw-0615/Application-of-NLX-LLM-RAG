# -*- coding: utf-8 -*-
"""RAG_Dataset_Explore_Step1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gs9V7cLOfRnZ95xGwKup7ctdke-evFDd

# Step 1: Dataset Exploration
- Based on the RAG Starter Code
"""

# pip install pymilvus[milvus_lite] ragas # Uncomment this line and paste to Powershell

# Load all required Libraries
import pandas as pd
import transformers, torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset

from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

"""## Read Passages from the Datasets and Drop rows if they are NA or empty"""

passages = pd.read_parquet("hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet")

print(passages.shape)
passages.head()

"""## Do EDA on the passage dataset
- You can try to find the maximum and minimum length of the passages before indexing (just a direction)
"""

# Code for EDA
passages['passage_length'] = passages['passage'].apply(len)

min_len = passages['passage_length'].min()
max_len = passages['passage_length'].max()
avg_len = passages['passage_length'].mean()
med_len = passages['passage_length'].median()

print(f"Minimum passage length: {min_len}")
print(f"Maximum passage length: {max_len}")
print(f"Average passage length: {avg_len:.2f}")
print(f"Median passage length: {med_len}")

"""## Tokenize Text and Generate Embeddings using Sentence Transformers"""

from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Encode Text
embeddings = embedding_model.encode(passages['passage'].tolist())

"""## Create Milvus Client and Insert your Embeddings to your DB
- Make sure you define a schema for your collection (Points will be deducted if you fail to define a proper schema with ids, passage text, embedding)
"""

# Define every column of your schema

id_ = FieldSchema(
    name="id", dtype=DataType.INT64, is_primary=True
)
passage = FieldSchema(
    name="passage", dtype=DataType.VARCHAR, max_length=65535
)
embedding = FieldSchema(
    name="embedding", dtype=DataType.FLOAT_VECTOR, dim=embeddings.shape[1]
)

schema = CollectionSchema(
    fields=[id_, passage, embedding], description="RAG Mini Wikipedia Collection"
)

client = MilvusClient("rag_wikipedia_mini.db")

# Create the Collection with Collection Name = "rag_mini". Make sure you define the schema variable while creating the collection
client.create_collection(collection_name="rag_mini", schema=schema)

"""**Convert your Pandas Dataframe to a list of dictionaries**
- The Dictionary at least have 3 keys [id, passage, embedding]
"""

rag_data = []
for row, embedding in zip(passages.itertuples(index=True), embeddings):
    rag_data.append({
        "id": row.Index,
        "passage": row.passage,
        "embedding": embedding
    })

# Code to insert the data to your DB
res = client.insert(collection_name="rag_mini", data=rag_data)

print(res)

"""- Do a Sanity Check on your database

**Do not delete the below line during your submission**
"""

print("Entity count:", client.get_collection_stats("rag_mini")["row_count"])
print("Collection schema:", client.describe_collection("rag_mini"))

"""**LLM platform connection test**"""
'''
!python --version
!pip show datasets sentence-transformers faiss-cpu # Uncomment these two lines and paste to Powershell to test
'''
from datasets import load_dataset

ds = load_dataset("rag-datasets/rag-mini-wikipedia", "question-answer")
print(ds)
print(ds["test"][0])

from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
vec = model.encode("Hello RAG")
print(vec.shape)   # Expected: (384,)

from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType

connections.connect("default", uri="milvus.db")  # Milvus
print("Milvus connected:", connections.has_connection("default"))

from openai import OpenAI
from google.colab import userdata

try:
    # Access the API key from Colab secrets
    #openai_api_key = userdata.get('OPENAI_API_KEY')
    openai_api_key = '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'

    # Initialize the OpenAI client with the API key
    client = OpenAI(api_key=openai_api_key)

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": "Test connection"}]
    )
    print(resp.choices[0].message)

except Exception as e:
    print(f"An error occurred: {e}")

# Step 1: Token length distribution (in addition to character length)

from transformers import AutoTokenizer
import matplotlib.pyplot as plt

# Tokenizer loading
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

# Count token numbers
passages["token_length"] = passages["passage"].apply(
    lambda x: len(tokenizer.encode(x, add_special_tokens=False))
)

# Stats printing
print("Minimum token length:", passages["token_length"].min())
print("Maximum token length:", passages["token_length"].max())
print("Average token length:", passages["token_length"].mean())
print("Median token length:", passages["token_length"].median())

# Plot
plt.figure(figsize=(8,5))
plt.hist(passages["token_length"], bins=50, color="skyblue", edgecolor="black")
plt.title("Distribution of Passage Token Lengths")
plt.xlabel("Token count per passage")
plt.ylabel("Frequency")
plt.show()

"""## Steps to Fetch Results
- Read the Question Dataset
- Clean the Question Dataset if necessary (Drop Questions with NaN etc.)
- Convert Each Query to a Vector Embedding (Use the same embedding model you used to embed your document)
- Try for a Single Question First
- Load Collection into Memory after creating Index for Search on your embedding field (This is an essential step before you can search in your db)
- Search and Fetch Top N Results
"""

import pandas as pd

queries = pd.read_parquet("hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet")
queries

query = queries["question"].iloc[0]                 # Your single query

query_embedding = embedding_model.encode(query)

print(query_embedding.shape)

"""### Create Index on the embedding column on your DB"""

index_params = MilvusClient.prepare_index_params()

# Add an index on the embedding field
...

# Create the index
try:
    ...
except Exception as e:
    print(f"Index creation result: {e}")

# Load collection into memory (required for search)
...
print("Collection loaded into memory")

# Search the db with your query embedding
output_ = ...

print(output_)

"""## Now get the Context
- Initially use the first passage ONLY as your context
- In Later Experiments, you must try at least 2 different passage selection strategies (Top 3 / Top 5 / Top 10) and pass to your prompt
"""

context = ...

"""**Develop your Prompt**"""

system_prompt = f""

prompt = f"""{system_prompt} \n Context: {context}: \n Question: {query} """
print(prompt)
