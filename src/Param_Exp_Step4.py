# -*- coding: utf-8 -*-
"""Param_Exp_Step4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nw7GMyC7GdqVLzLhvXtTReNe2YZb6kKT

# Step 4 â€” Parameter Comparison Experiment

## 0) Configs
"""

CFG = {
    "models": [
        ("sentence-transformers/all-MiniLM-L6-v2", "miniLM6v2", 384),
        ("sentence-transformers/all-mpnet-base-v2", "mpnetv2", 512),
    ],
    "chunk_max_chars": 600, # Keep in accordance with Step3, may change to experiment chunking size later
    "retrieval_topk": [3, 5, 10],
    "selection_strategies": ["concat", "mmr"],
    "max_ctx_chars": 2000,
    "n_samples": None,   # Set to 200 to test
    "batch_size": 64,
}
print(CFG)

"""## 1) Install dependencies"""

# !pip install -q datasets evaluate transformers sentence-transformers faiss-cpu pandas numpy matplotlib
# Uncomment above line if running in Colab

from datasets import load_dataset
import evaluate
import numpy as np
import pandas as pd
import time
from typing import List, Tuple
from sentence_transformers import SentenceTransformer
import faiss
import matplotlib.pyplot as plt

"""## 2) Answer generator using local Flan-T5 Transformer"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

tok = AutoTokenizer.from_pretrained("google/flan-t5-base")
mdl = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
gen_pipe = pipeline("text2text-generation", model=mdl, tokenizer=tok)

def generate_answer(prompt, max_new_tokens=128):
    out = gen_pipe(prompt, max_new_tokens=max_new_tokens)[0]["generated_text"]
    return out.strip()

"""## 3) Load `text-corpus` and `question-answer` subsets from RAG-Wikipedia"""

from datasets import load_dataset

corpus_ds = load_dataset("rag-datasets/rag-mini-wikipedia", name="text-corpus")
print("Keys in corpus dataset:", corpus_ds.keys())

qa_ds = load_dataset("rag-datasets/rag-mini-wikipedia", name="question-answer")
print("Keys in qa dataset:", qa_ds.keys())

corpus = corpus_ds["passages"]
qa = qa_ds["test"]

print("Corpus size:", len(corpus))
print("QA size:", len(qa))

"""## 4) Text chunking"""

def simple_chunks(text: str, max_chars: int) -> List[str]:
    if not text:
        return []
    return [text[i:i+max_chars] for i in range(0, len(text), max_chars)]

def build_docs(corpus_split, max_chars: int):
    docs = []
    for i, row in enumerate(corpus_split):
        t = row.get("text") or row.get("passage") or ""
        chs = simple_chunks(t, max_chars)
        for j, ch in enumerate(chs):
            docs.append({"id": f"{i}-{j}", "text": ch})
    return docs

docs = build_docs(corpus, CFG["chunk_max_chars"])
print("Total chunks:", len(docs))

"""## 5) Index building"""

def build_index(model_id: str, expected_dim: int):
    model = SentenceTransformer(model_id)
    texts = [d["text"] for d in docs]
    X = model.encode(texts, batch_size=CFG["batch_size"], show_progress_bar=True, normalize_embeddings=True, truncate=True) # Truncate a few overly long text segments to fit in model
    X = X.astype("float32")
    index = faiss.IndexFlatIP(X.shape[1])
    index.add(X)
    return {"model": model, "index": index, "emb": X}

per_model = {}
for mid, short, dim in CFG["models"]:
    print("Building index for:", short)
    per_model[short] = build_index(mid, dim)

"""## 6) Search and MMR"""

def faiss_search(bundle, query: str, top_k: int):
    qv = bundle["model"].encode([query], normalize_embeddings=True).astype("float32")
    D, I = bundle["index"].search(qv, top_k)
    return list(I[0])

def mmr_rerank(bundle, query: str, candidates: List[int], select_k=5, lambda_div=0.5):
    emb = bundle["emb"]
    qv = bundle["model"].encode([query], normalize_embeddings=True).astype("float32")[0]
    selected = []
    sims_q = emb[candidates] @ qv
    while len(selected) < min(select_k, len(candidates)):
        if not selected:
            j = int(np.argmax(sims_q))
        else:
            sims_sel = emb[candidates] @ emb[selected].T
            max_sim = sims_sel.max(axis=1)
            mmr_scores = lambda_div * sims_q - (1 - lambda_div) * max_sim
            j = int(np.argmax(mmr_scores))
        selected.append(candidates[j])
        candidates.pop(j); sims_q = np.delete(sims_q, j, axis=0)
    return selected

def build_context(bundle, query, top_k, strategy, budget=2000):
    recall_k = max(top_k*3, top_k) if strategy=="mmr" else top_k
    hits = faiss_search(bundle, query, recall_k)
    if strategy=="concat":
        chosen = hits[:top_k]
    else:
        chosen = mmr_rerank(bundle, query, hits, select_k=top_k)
    parts, used = [], 0
    for idx in chosen:
        text = docs[idx]["text"]
        if used + len(text) > budget:
            parts.append(text[:budget-used])
            break
        parts.append(text); used += len(text)
    return "\n\n".join(parts)

"""## 7) Prompt definition and evaluation
- Use `Persona` prompt which generates best exact-match and F1 in Step 3
- Use Huggingface SQuAD metrics to evaluate
"""

def persona_prompt(context, question):
    return f"You are a concise encyclopedia editor. Use ONLY the context.\n\nContext:\n{context}\n\nQuestion: {question}\nAnswer:"

squad = evaluate.load("squad")

def run_eval(bundle_key, top_k, strategy, n_samples=None):
    bundle = per_model[bundle_key]
    total = len(qa) if n_samples is None else min(n_samples, len(qa))
    preds, refs = [], []
    for i in range(total):
        q = qa[i]["question"]
        gold = qa[i]["answer"] if "answer" in qa[i] else qa[i]["answers"]
        gold_text = gold if isinstance(gold, str) else gold[0]
        ctx = build_context(bundle, q, top_k, strategy)
        prompt = persona_prompt(ctx, q)
        ans = generate_answer(prompt)
        preds.append({"id": str(i), "prediction_text": ans})
        refs.append({"id": str(i), "answers": {"text": [gold_text], "answer_start": [0]}})
    metrics = squad.compute(predictions=preds, references=refs)
    return metrics

"""## 8) Top-K grid search
- Search under k = 3/5/10
"""

rows = []
for mid, short, dim in CFG["models"]:
    for k in CFG["retrieval_topk"]:
        for strat in CFG["selection_strategies"]:
            print(f"Running {short}, top_k={k}, strat={strat}")
            res = run_eval(short, k, strat, n_samples=CFG["n_samples"] or 50)  # Default 50 quick experiments
            row = {"model": short, "top_k": k, "strategy": strat, **res}
            print(" ->", row)
            rows.append(row)

df = pd.DataFrame(rows)
print(df)

"""## 9) Visualization of results"""

for (m, s), sub in df.groupby(["model","strategy"]):
    plt.plot(sub["top_k"], sub["f1"], marker="o", label=f"{m}-{s}")
plt.xlabel("top_k")
plt.ylabel("F1")
plt.title("F1 vs top_k")
plt.legend()
plt.show()