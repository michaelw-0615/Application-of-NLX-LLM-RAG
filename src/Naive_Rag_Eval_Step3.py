# -*- coding: utf-8 -*-
"""Naive_RAG_Eval_Step3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1khCF20ZD5meNRx6iePe0ItH-7duVdKTg

# Step 3 — Evaluation Phase I (Top‑1 Evidence)  
**Reuses Step 2 functions via `%run`**

This notebook is designed to be saved anywhere and run **after** your Step 2 notebook.  
It reuses `retrieve(query, top_k=1)` and your vector store setup from Step 2, then evaluates multiple prompting strategies with **SQuAD F1/EM**.

**What this notebook does**
1. Configure the path to your **Step 2** notebook.
2. `%run` Step 2 to load the vector store and the `retrieve()` function.
3. Load the `question-answer` split from *rag-mini-wikipedia*.
4. Define three prompt strategies (Instruction / CoT / Persona).
5. Generate answers using a local Transformers model (or OpenAI if an API key is available).
6. Compute **exact_match** and **f1** using the Hugging Face `squad` metric.
7. Print the best strategy and its scores.

> If stronger decoupling needed, move Step 2 core into a Python module (e.g., `rag_core.py`) and import it here.

## 0) Configure the Step 2 notebook path

- Set `STEP2_NOTEBOOK` to the **absolute path** of your Step 2 notebook.  
- Example: `/path/to/Naive_RAG_Milvus_Step2.ipynb` (Linux/Mac) or `C:\\path\\to\\Naive_RAG_Milvus_Step2.ipynb` (Windows).
"""

# >>>> EDIT THIS TO MATCH YOUR FILE SYSTEM <<<<
STEP2_NOTEBOOK = "My Drive/Colab Notebooks/Naive_RAG_Milvus_Step2.ipynb"  # change to your actual path
print("Using Step 2 notebook at:", STEP2_NOTEBOOK)

"""
## 1) Bring in Step 2 (vector store & retrieve) via `%run`

- This cell executes your Step 2 notebook so the current kernel has `retrieve()` and the vector store loaded.
- If Step 2 rebuilds the index on run, that's fine; otherwise it will reuse whatever is already persisted (e.g., Milvus Lite `milvus.db`).
"""

# Commented out IPython magic to ensure Python compatibility.
import os

# Execute Step 2 so we can reuse retrieve() and the embedding/vector store setup
notebook_path = os.path.join('/content', STEP2_NOTEBOOK)

if not os.path.exists(notebook_path):
    raise FileNotFoundError(f"Step 2 notebook not found at: {notebook_path}")

# %run "{notebook_path}"

# Sanity: check retrieve is available
assert 'retrieve' in globals(), "retrieve() not found; please confirm the Step 2 notebook path."
print("Step 2 executed. Functions available:", [n for n in ('retrieve','answer_with_context','model','col') if n in globals()])

# Commented out IPython magic to ensure Python compatibility.
# %pip install pymilvus[milvus_lite]

from google.colab import drive
drive.mount('/content/drive')

import os

# >>>> EDIT THIS TO MATCH YOUR FILE SYSTEM <<<<
# Assuming your notebook is in the root of "My Drive"
STEP2_NOTEBOOK = "/content/drive/My Drive/Colab Notebooks/Naive_RAG_Milvus_Step2.ipynb"  # change to your actual path
print("Using Step 2 notebook at:", STEP2_NOTEBOOK)

# Commented out IPython magic to ensure Python compatibility.
import os

# Execute Step 2 so we can reuse retrieve() and the embedding/vector store setup
notebook_path = STEP2_NOTEBOOK # Use the updated path directly

if not os.path.exists(notebook_path):
    raise FileNotFoundError(f"Step 2 notebook not found at: {notebook_path}")

# %run "{notebook_path}"

# Sanity: check retrieve is available
assert 'retrieve' in globals(), "retrieve() not found; please confirm the Step 2 notebook path."
print("Step 2 executed. Functions available:", [n for n in ('retrieve','answer_with_context','model','col') if n in globals()])

"""
## 2) Install and import evaluation dependencies

We use:  
- `datasets` to load *rag-mini-wikipedia*  
- `evaluate` for the **SQuAD** metric  
- `transformers` for a local generation baseline (fallback to OpenAI if configured)
"""

import sys, subprocess, pkgutil

def ensure(pkg, pip_name=None):
    pip_name = pip_name or pkg
    if pkgutil.find_loader(pkg) is None:
        print(f"Installing: {pip_name}")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", pip_name])
    else:
        print(f"OK: {pkg}")

ensure("datasets", "datasets")
ensure("evaluate", "evaluate")
ensure("transformers", "transformers")

from datasets import load_dataset
import evaluate
import numpy as np

"""## 3) Load the QA split and the SQuAD metric

"""

ds = load_dataset("rag-datasets/rag-mini-wikipedia", "question-answer")
qa = ds["test"] # Use the correct split name 'test'
squad = evaluate.load("squad")

print("QA size:", len(qa))
print("Sample QA:", {k: qa[0][k] for k in qa[0].keys() if k in ("question","answer","answers")})

"""
## 4) Define prompting strategies

We compare three simple, reproducible templates:
- **Instruction**
- **Chain-of-Thought (CoT)** (concise: asks to reason step by step)
- **Persona** (concise editor style)
"""

def build_prompt_instruction(context, question):
    return (
        "Answer STRICTLY using the context. If insufficient, reply 'I don't know.'\n\n"
        f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"
    )

def build_prompt_cot(context, question):
    return (
        "You are a careful analyst. Use ONLY the context. "
        "If insufficient, say 'I don't know.' Carefully plan your reasoning step by step.\n\n"
        f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"
    )

def build_prompt_persona(context, question):
    return (
        "You are a concise encyclopedia editor. Use ONLY the context. "
        "If insufficient, say 'I don't know.' Keep answers factual and brief.\n\n"
        f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"
    )

PROMPTS = {
    "instruction": build_prompt_instruction,
    "cot": build_prompt_cot,
    "persona": build_prompt_persona,
}

"""
## 5) Generation backend (local Transformers by default; OpenAI optional)

- Default uses a small local model (`google/flan-t5-base`) to keep the notebook self-contained.
- If you have `OPENAI_API_KEY` in env, you can switch to `gpt-4o-mini` by un-commenting the OpenAI block.
"""

# Option A: local Transformers (default)

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

try:
    _tok = AutoTokenizer.from_pretrained("google/flan-t5-base")
    _mdl = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base")
    pipe = pipeline("text2text-generation", model=_mdl, tokenizer=_tok)
    def generate_answer(prompt, max_new_tokens=128, temperature=0.0):
        out = pipe(prompt, max_new_tokens=max_new_tokens)[0]["generated_text"]
        return out.strip()
    print("Using local Transformers: google/flan-t5-base")
except Exception as e:
    print("Local Transformers unavailable, you may switch to OpenAI below if you have an API key.", e)
'''
    # --- Option B: OpenAI (uncomment to use) ---
from openai import OpenAI
import os
client = OpenAI(api_key='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')
def generate_answer(prompt, max_new_tokens=128, temperature=0.2):
    resp = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role":"user","content":prompt}],
        temperature=temperature, max_tokens=max_new_tokens
    )
    return resp.choices[0].message.content.strip()
'''

"""
## 6) Enforce the Step 3 constraint: Top‑1 evidence only

We call `retrieve(query, top_k=1)` from Step 2 and pass the *single* retrieved chunk into the prompt.
"""

def get_top1_context(query):
    hits = retrieve(query, top_k=1)  # <-- strict top-1 evidence
    return hits[0][1] if hits else ""

"""
## 7) Evaluation loop (compute SQuAD F1 / EM)
"""

def evaluate_strategy(prompt_name, n_samples=None):
    build = PROMPTS[prompt_name]
    preds, refs = [], []
    total = len(qa) if n_samples is None else min(n_samples, len(qa))

    for i in range(total):
        q = qa[i]["question"]
        gold = qa[i]["answer"] if "answer" in qa[i] else qa[i]["answers"]
        gold_text = gold if isinstance(gold, str) else gold[0]

        ctx = get_top1_context(q)
        prompt = build(ctx, q)
        pred = generate_answer(prompt)

        preds.append({"id": str(i), "prediction_text": pred})
        refs.append({"id": str(i), "answers": {"text": [gold_text], "answer_start": [0]}})

    return evaluate.load("squad").compute(predictions=preds, references=refs), preds

# quick sanity on a subset first
for name in PROMPTS:
    m, _ = evaluate_strategy(name, n_samples=20)
    print(name, m)

"""
## 8) Full run & select best strategy
"""

results = {}
for name in PROMPTS:
    metrics, _ = evaluate_strategy(name, n_samples=None)  # full set
    results[name] = metrics

print("All results:", results)
best_by_f1 = max(results.items(), key=lambda kv: kv[1]["f1"])
best_by_em = max(results.items(), key=lambda kv: kv[1]["exact_match"])
print("Best F1:", best_by_f1)
print("Best EM:", best_by_em)

"""
## 9) Notes & Tips for Reproduction
- Ensure the **Step 2** notebook builds or loads the vector index before running this notebook.
- Keep generation temperature low (0–0.2) for reproducibility.
- Save results (e.g., to `results/step3_baseline.json`) for later comparison in Step 4–6.
"""